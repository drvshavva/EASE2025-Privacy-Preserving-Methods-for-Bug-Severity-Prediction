{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import (AdamW, RobertaModel)\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "from src.evalution import evaluate_result\n"
   ],
   "metadata": {
    "id": "vONjsE4Jk-o8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_PATH = \"/../../../datasets\"\n",
    "CLIENT_FILES = [f\"{DATA_PATH}/llm/client_0.jsonl\", f\"{DATA_PATH}/llm/client_1.jsonl\", f\"{DATA_PATH}/llm/client_2.jsonl\"]\n",
    "\n",
    "# Hyperparametreler\n",
    "BATCH_SIZE = 16\n",
    "BLOCK_SIZE = 512\n",
    "EPOCHS = 5\n",
    "NUM_ROUNDS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "\n",
    "# Tokenizer yükleme\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "metadata": {
    "id": "aX72BFayqhk4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size=512):\n",
    "        self.examples = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                code_tokens = tokenizer.tokenize(data[\"code_no_comment\"])[:block_size - 2]\n",
    "                input_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + code_tokens + [tokenizer.eos_token])\n",
    "                padding_length = block_size - len(input_ids)\n",
    "                input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                self.examples.append((torch.tensor(input_ids), torch.tensor(data[\"label\"])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n"
   ],
   "metadata": {
    "id": "aqJzBGPFllfS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CodeBERTModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=input_ids.ne(1))[0]\n",
    "        cls_output = self.dropout(outputs[:, 0, :])\n",
    "        logits = self.classifier(cls_output)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, labels) if labels is not None else None\n",
    "        return loss, logits"
   ],
   "metadata": {
    "id": "K2BJu47RtQLf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_loader, optimizer, epochs=1, checkpoint_interval=1, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)  \n",
    "\n",
    "\n",
    "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".bin\")])\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
    "        print(f\"En son checkpoint yüklendi: {latest_checkpoint}\")\n",
    "        model.load_state_dict(torch.load(latest_checkpoint))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            input_ids, labels = [b.to(DEVICE) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(input_ids, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}\")\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.bin\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint kaydedildi: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, labels = [b.to(DEVICE) for b in batch]\n",
    "            loss_batch, logits = model(input_ids, labels)\n",
    "            loss += loss_batch.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        metrics = evaluate_result(np.array(all_labels), np.array(all_preds), None)\n",
    "    return loss / total, correct / total, metrics['eval_f1'], metrics['eval_precision'], metrics['eval_recall'], \\\n",
    "        metrics['eval_mcc'], metrics['eval_cohen_kappa_score'], metrics['eval_gmean']"
   ],
   "metadata": {
    "id": "bQbT-vUcqgKD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CodeBERTClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, train_loader, test_loader):\n",
    "        self.model = CodeBERTModel(MODEL_NAME).to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        train(self.model, self.train_loader, self.optimizer, epochs=EPOCHS)\n",
    "        return self.get_parameters(config={}), len(self.train_loader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy, f1, precision, recall, mcc, kappa, gmean = test(self.model, self.test_loader)\n",
    "        return float(loss), len(self.test_loader.dataset), {\"accuracy\": float(accuracy), \"loss\": float(loss),\n",
    "                                                            \"f1\": float(f1), \"precision\": float(precision),\n",
    "                                                            \"recall\": float(recall), \"mcc\": float(mcc),\n",
    "                                                            \"kappa\": float(kappa), \"gmean\": float(gmean)}\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(cid):\n",
    "    train_dataset = CodeDataset(CLIENT_FILES[int(cid)], tokenizer, BLOCK_SIZE)\n",
    "    test_dataset = CodeDataset(f\"{DATA_PATH}/data/test_scaled.jsonl\", tokenizer, BLOCK_SIZE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    return CodeBERTClient(cid, train_loader, test_loader)\n"
   ],
   "metadata": {
    "id": "viSzVRZ1lsF_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "MODEL_SAVE_PATH = \"codebert_federated_final.bin\"\n",
    "\n",
    "\n",
    "def save_final_model(parameters):\n",
    "    \"\"\"Federated modelin son ağırlıklarını kaydeder.\"\"\"\n",
    "    if parameters is None:\n",
    "        print(\"Son model parametreleri alınamadı! Kaydetme işlemi iptal edildi.\")\n",
    "        return\n",
    "\n",
    "    model = CodeBERTModel(MODEL_NAME).to(\"cpu\")  # Modeli CPU'ya al\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), parameters)})\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Federated model başarıyla kaydedildi: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "\n",
    "def on_fit_config_fn(server_round):\n",
    "    if server_round == NUM_ROUNDS:\n",
    "        print(f\"Son tur {NUM_ROUNDS} tamamlandı, modelin kaydedilmesi için işaret bırakılıyor...\")\n",
    "    return {}\n",
    "\n",
    "\n",
    "def weighted_average(metrics):\n",
    "    \"\"\"Federated öğrenmede ağırlıklı ortalama hesaplar.\"\"\"\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
    "    f1s = [num_examples * m[\"f1\"] for num_examples, m in metrics]\n",
    "    pres = [num_examples * m[\"precision\"] for num_examples, m in metrics]\n",
    "    recalls = [num_examples * m[\"recall\"] for num_examples, m in metrics]\n",
    "    mccs = [num_examples * m[\"mcc\"] for num_examples, m in metrics]\n",
    "    kappas = [num_examples * m[\"kappa\"] for num_examples, m in metrics]\n",
    "    gmeans = [num_examples * m[\"gmean\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples), \"loss\": sum(losses) / sum(examples),\n",
    "            \"f1\": sum(f1s) / sum(examples), \"precision\": sum(pres) / sum(examples), \"mcc\": sum(mccs) / sum(examples),\n",
    "            \"recall\": sum(recalls) / sum(examples), \"kappa\": sum(kappas) / sum(examples),\n",
    "            \"gmean\": sum(gmeans) / sum(examples)}\n",
    "\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  \n",
    "    fraction_evaluate=1.0,  \n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    on_fit_config_fn=on_fit_config_fn\n",
    ")\n",
    "\n",
    "\n",
    "hist = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=3,  # 3 farklı istemci\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),  # 1 tur federated eğitim\n",
    "    strategy=strategy,\n",
    "    client_resources={\"num_cpus\": 1, \"num_gpus\": 1},  \n",
    "    ray_init_args={\"log_to_driver\": False, \"num_cpus\": 1, \"num_gpus\": 1}\n",
    ")\n",
    "\n",
    "#if hist.metrics_distributed and \"parameters\" in hist.metrics_distributed[-1]:\n",
    "#    final_parameters = hist.metrics_distributed[-1][\"parameters\"]\n",
    "#    save_final_model(final_parameters)\n",
    "#    print(\"Federated model başarıyla kaydedildi!\")\n",
    "#else:\n",
    "#    print(\"Son turdan sonra model parametreleri alınamadı!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHkKRqx5l6X1",
    "outputId": "d51b6050-2bd5-4d01-c004-ab89472f5781"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[93mWARNING \u001B[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001B[92mINFO \u001B[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
      "2025-03-10 17:21:36,282\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001B[92mINFO \u001B[0m:      Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'CPU': 1.0, 'memory': 32665718784.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 16332859392.0, 'accelerator_type:T4': 1.0, 'GPU': 1.0}\n",
      "\u001B[92mINFO \u001B[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001B[92mINFO \u001B[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 1}\n",
      "\u001B[92mINFO \u001B[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
      "\u001B[92mINFO \u001B[0m:      [INIT]\n",
      "\u001B[92mINFO \u001B[0m:      Requesting initial parameters from one random client\n",
      "\u001B[92mINFO \u001B[0m:      Received initial parameters from one random client\n",
      "\u001B[92mINFO \u001B[0m:      Starting evaluation of initial global parameters\n",
      "\u001B[92mINFO \u001B[0m:      Evaluation returned no results (`None`)\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 1]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[93mWARNING \u001B[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 2]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 3]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 4]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 5]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Son tur 5 tamamlandı, modelin kaydedilmesi için işaret bırakılıyor...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [SUMMARY]\n",
      "\u001B[92mINFO \u001B[0m:      Run finished 5 round(s) in 6177.94s\n",
      "\u001B[92mINFO \u001B[0m:      \tHistory (loss, distributed):\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 1: 0.0636893023413966\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 2: 0.08374084900337386\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 3: 0.09940563212798173\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 4: 0.11366700011479902\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 5: 0.10415759033326846\n",
      "\u001B[92mINFO \u001B[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001B[92mINFO \u001B[0m:      \t{'accuracy': [(1, 0.7131474103585658),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (2, 0.7350597609561753),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (3, 0.7310756972111554),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (4, 0.7390438247011952),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (5, 0.749003984063745)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'f1': [(1, 0.6694530387542599),\n",
      "\u001B[92mINFO \u001B[0m:      \t        (2, 0.7074579068912084),\n",
      "\u001B[92mINFO \u001B[0m:      \t        (3, 0.7035849833559386),\n",
      "\u001B[92mINFO \u001B[0m:      \t        (4, 0.7089379603934003),\n",
      "\u001B[92mINFO \u001B[0m:      \t        (5, 0.7253502107130129)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'gmean': [(1, 0.6859382610085215),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (2, 0.7249497470937385),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (3, 0.723709149356258),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (4, 0.7265498856798269),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (5, 0.7404879984199613)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'kappa': [(1, 0.44476703995575845),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (2, 0.5084482466667157),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (3, 0.5041666971517205),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (4, 0.513479521780303),\n",
      "\u001B[92mINFO \u001B[0m:      \t           (5, 0.5373948657939004)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'loss': [(1, 0.0636893023413966),\n",
      "\u001B[92mINFO \u001B[0m:      \t          (2, 0.08374084900337386),\n",
      "\u001B[92mINFO \u001B[0m:      \t          (3, 0.09940563212798173),\n",
      "\u001B[92mINFO \u001B[0m:      \t          (4, 0.11366700011479902),\n",
      "\u001B[92mINFO \u001B[0m:      \t          (5, 0.10415759033326846)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'mcc': [(1, 0.48188956886027307),\n",
      "\u001B[92mINFO \u001B[0m:      \t         (2, 0.5288930476643254),\n",
      "\u001B[92mINFO \u001B[0m:      \t         (3, 0.5222972273609547),\n",
      "\u001B[92mINFO \u001B[0m:      \t         (4, 0.5374573908166637),\n",
      "\u001B[92mINFO \u001B[0m:      \t         (5, 0.5555148763986051)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'precision': [(1, 0.700549897473936),\n",
      "\u001B[92mINFO \u001B[0m:      \t               (2, 0.7199783571704168),\n",
      "\u001B[92mINFO \u001B[0m:      \t               (3, 0.713001580659985),\n",
      "\u001B[92mINFO \u001B[0m:      \t               (4, 0.728814312172883),\n",
      "\u001B[92mINFO \u001B[0m:      \t               (5, 0.7374325635313324)],\n",
      "\u001B[92mINFO \u001B[0m:      \t 'recall': [(1, 0.7131474103585658),\n",
      "\u001B[92mINFO \u001B[0m:      \t            (2, 0.7350597609561753),\n",
      "\u001B[92mINFO \u001B[0m:      \t            (3, 0.7310756972111554),\n",
      "\u001B[92mINFO \u001B[0m:      \t            (4, 0.7390438247011952),\n",
      "\u001B[92mINFO \u001B[0m:      \t            (5, 0.749003984063745)]}\n",
      "\u001B[92mINFO \u001B[0m:      \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model(test_file):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = CodeBERTModel(MODEL_NAME).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(\"/content/checkpoints/checkpoint_epoch_5.bin\", map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = CodeDataset(test_file, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    correct, total = 0, 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, labels = [b.to(DEVICE) for b in batch]\n",
    "            loss, logits = model(input_ids, labels)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    res = evaluate_result(all_labels, all_preds, None)\n",
    "    print(res)"
   ],
   "metadata": {
    "id": "DbLfGzdzxMuX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_model(f\"{DATA_PATH}/data/test_scaled.jsonl\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfExJFo2uc3k",
    "outputId": "d01d8691-cd14-4d12-ee1a-fe435771a24e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-21-ea8510ced5b0>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/content/checkpoints/checkpoint_epoch_5.bin\", map_location=DEVICE))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_f1': 0.7213580841469288, 'eval_f1_perclass': [0.5714285714285714, 0.7980769230769231, 0.8958333333333334, 0.51], 'eval_acc': 0.7310756972111554, 'eval_precision': 0.7202474689904282, 'eval_recall': 0.7310756972111554, 'eval_ROC-UAC': 0.0, 'eval_mcc': 0.5244824990263965, 'eval_cohen_kappa_score': 0.5197363758769754, 'eval_gmean': 0.7391787397987591}\n"
     ]
    }
   ]
  }
 ]
}
