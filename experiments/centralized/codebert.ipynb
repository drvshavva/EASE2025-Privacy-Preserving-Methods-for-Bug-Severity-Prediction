{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lv5LllMG8Wb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, matthews_corrcoef, \\\n",
    "    cohen_kappa_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import (AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaTokenizer, RobertaModel)\n",
    "from src.evalution import evaluate_result\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size):\n",
    "        self.examples = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                code_tokens = tokenizer.tokenize(data[\"code_no_comment\"])[:block_size - 2]\n",
    "                input_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + code_tokens + [tokenizer.eos_token])\n",
    "                padding_length = block_size - len(input_ids)\n",
    "                input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                self.examples.append((torch.tensor(input_ids), torch.tensor(data[\"label\"])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n"
   ],
   "metadata": {
    "id": "uonLRI8VHNwA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CodeBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        outputs = self.encoder(input_ids, attention_mask=input_ids.ne(1))[0]\n",
    "        cls_output = self.dropout(outputs[:, 0, :])\n",
    "        logits = self.classifier(cls_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss, logits\n"
   ],
   "metadata": {
    "id": "C6GvkDorHWys"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(train_dataset, model, tokenizer, args):\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=args[\"batch_size\"])\n",
    "    optimizer = AdamW(model.parameters(), lr=args[\"lr\"], eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                                num_training_steps=len(train_dataloader) * args[\"epochs\"])\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            input_ids, labels = [b.to(args[\"device\"]) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(input_ids, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_dataloader)}\")\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "t5F7YD91HYNS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(eval_dataset, model, args):\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=SequentialSampler(eval_dataset), batch_size=args[\"batch_size\"])\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids, labels = [b.to(args[\"device\"]) for b in batch]\n",
    "            _, logits = model(input_ids)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    metrics = evaluate_result(all_labels, all_preds, None)\n",
    "    print(metrics)"
   ],
   "metadata": {
    "id": "r08aOUW6HbaH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    DATA = \"/../../datasets\"\n",
    "    args = {\n",
    "        \"model_name\": \"microsoft/codebert-base\",\n",
    "        \"train_file\": f\"{DATA}/data/train_scaled.jsonl\",\n",
    "        \"eval_file\": f\"{DATA}/data/test_scaled.jsonl\",\n",
    "        \"block_size\": 512,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 20,\n",
    "        \"lr\": 2e-5,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args[\"model_name\"])\n",
    "    train_dataset = CodeDataset(args[\"train_file\"], tokenizer, args[\"block_size\"])\n",
    "    eval_dataset = CodeDataset(args[\"eval_file\"], tokenizer, args[\"block_size\"])\n",
    "    model = CodeBERTModel(args[\"model_name\"], num_labels=4).to(args[\"device\"])\n",
    "    model = train_model(train_dataset, model, tokenizer, args)\n",
    "    evaluate_model(eval_dataset, model, args)\n",
    "    torch.save(model.state_dict(), \"codebert_finetuned.bin\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE-XT_wlHsQ4",
    "outputId": "5cda76ea-d6a8-40b7-f778-c43d34066f91"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 151/151 [03:48<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Loss: 0.9959236851196416\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 Loss: 0.7903857144298932\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 Loss: 0.6280412535793733\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 Loss: 0.4865910896402321\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 Loss: 0.31222406984461065\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 Loss: 0.21255303164388958\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 Loss: 0.1428002743852257\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 151/151 [03:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 Loss: 0.11299956439136966\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 Loss: 0.08888575891094493\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 Loss: 0.06605189614034943\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 11 Loss: 0.053451008340713896\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 151/151 [03:47<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 12 Loss: 0.0432295185584723\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 13 Loss: 0.035073446555507204\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14 Loss: 0.029018199578581365\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 15 Loss: 0.02660638009913463\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 16 Loss: 0.026716479555154774\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 17 Loss: 0.022253247352613026\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 18 Loss: 0.0208825362770701\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 151/151 [03:46<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 19 Loss: 0.0172816794539111\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 151/151 [03:47<00:00,  1.50s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 20 Loss: 0.01422092974855139\n",
      "{'eval_f1': 0.7268955911596828, 'eval_f1_perclass': [0.6136363636363636, 0.8111455108359134, 0.9361702127659575, 0.4659090909090909], 'eval_acc': 0.7450199203187251, 'eval_precision': 0.7355248824396095, 'eval_recall': 0.7450199203187251, 'eval_ROC-UAC': 0.0, 'eval_mcc': 0.5465303727366377, 'eval_cohen_kappa_score': 0.5318733516923839, 'eval_gmean': 0.7377535169269442}\n"
     ]
    }
   ]
  }
 ]
}
